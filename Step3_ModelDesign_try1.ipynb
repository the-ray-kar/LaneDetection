{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "#https://pytorch.org/hub/ultralytics_yolov5/\n",
    "\n",
    "# with open(\"CUDatamap_line2.pkl\",\"wb\") as f:\n",
    "#     pickle.dump(data_line_map,f) #Store the datamap for further processig\n",
    "# with open(\"CUDatamap_coef.pkl\",\"wb\") as f:\n",
    "#     pickle.dump(data_coef_map,f) #Store the datamap for further processig\n",
    "\n",
    "with open(\"CUDatamap_line2.pkl\",\"rb\") as f:\n",
    "    data_line_map = pickle.load(f)\n",
    "with open(\"CUDatamap_stline2.pkl\",\"rb\") as f:\n",
    "    data_stline_map = pickle.load(f)\n",
    "\n",
    "with open(\"CUDatamap_coef.pkl\",\"rb\") as f:\n",
    "    data_coef_map = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "y_col = np.linspace(400,580,19)\n",
    "\n",
    "class LaneDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_line_map,data_stline_map,scale=None):\n",
    "        \n",
    "        self.new_data_keys = list(data_line_map.keys())\n",
    "        self.data_line_map = data_line_map\n",
    "        self.data_stline_map = data_stline_map\n",
    "        self.scale = scale\n",
    "        image_loc = self.new_data_keys[0]\n",
    "        image = cv2.imread(image_loc)\n",
    "        height,width,channel = image.shape\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "\n",
    "  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.new_data_keys)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def get_lane_angle(self,myline):\n",
    "        global y_col #its actually row\n",
    "        row = y_col[-1]\n",
    "        col = myline[-1]\n",
    "        \n",
    "        new_col = col - self.width//2\n",
    "        new_row = row + self.height\n",
    "\n",
    "        angle = -np.arctan2(new_col,new_row)\n",
    "        # if angle<0:\n",
    "        #     angle = 2*np.pi+angle\n",
    "        \n",
    "        return angle\n",
    "\n",
    "    def get_lane_bins(self,stline):\n",
    "        angle = self.get_lane_angle(stline)\n",
    "        angle = np.clip(angle,-1,1) \n",
    "        mybin = int(round(angle/0.125))+8 #17 bins -1 to 1 as 0 to 17 \n",
    "\n",
    "        return mybin,angle    \n",
    "\n",
    "        \n",
    "    def get_regression_confidence_mat(self,stlines,lines):\n",
    "        regression_mat = np.zeros((19,17),dtype=np.float32) #19 rows and 17 for 17 bins\n",
    "        confidence_mat = np.zeros((17,),dtype=np.float32)\n",
    "        for stline,line in zip(stlines,lines):\n",
    "            mybin,angle = self.get_lane_bins(stline=stline)\n",
    "            regression_mat[:,mybin] = line\n",
    "            confidence_mat[mybin] = 1\n",
    "        return regression_mat.flatten(),confidence_mat\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        image_loc = self.new_data_keys[idx]\n",
    "        lines = self.data_line_map[image_loc] #Since\n",
    "        image = cv2.imread(image_loc)\n",
    "        stlines = self.data_stline_map[image_loc]\n",
    "\n",
    "        \n",
    "        #visualize_bundle = (image,lines,stlines,idx) #use only for visualization\n",
    "        \n",
    "\n",
    "        image_tensor = torch.tensor(image,dtype=torch.float32)/255 #convert to tensor and normalise\n",
    "        image_tensor2 = image_tensor.permute(2,0,1) #bring the channel to front\n",
    "\n",
    "        regression_mat,confidence_mat  = self.get_regression_confidence_mat(stlines,lines)\n",
    "        regression_mat_tensor = torch.tensor(regression_mat,dtype=torch.float32)\n",
    "        confidence_mat_tensor = torch.tensor(confidence_mat,dtype=torch.float32)\n",
    "\n",
    "        #print(image_loc,sum(regression_mat))\n",
    "        \n",
    "\n",
    "        return image_tensor2,regression_mat_tensor,confidence_mat_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshayd/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/akshayd/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/akshayd/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [10:15<00:00, 899kB/s]    \n"
     ]
    }
   ],
   "source": [
    "# Load the VGG16 model from torchvision\n",
    "vgg16 = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vgg16,\"vgg16.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8756090160951"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = torch.load(\"vgg16.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8756090053527"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_net_features = nn.Sequential(\n",
    "            vgg16.features[0],  # Conv1_1\n",
    "            vgg16.features[1],  # ReLU1_1\n",
    "            vgg16.features[2],  # Conv1_2\n",
    "            vgg16.features[3],  # ReLU1_2\n",
    "            vgg16.features[4],  # \n",
    "            vgg16.features[5],  # \n",
    "            vgg16.features[6],  # \n",
    "            vgg16.features[7],  # \n",
    "            vgg16.features[8],  # \n",
    "            vgg16.features[9],  # \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lane_dataset = LaneDataset(data_line_map=data_line_map,data_stline_map=data_stline_map)\n",
    "dataloader = DataLoader(lane_dataset, batch_size=1,shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for batch in dataloader:\n",
    "    image_tensor,regression_mat_tensor,confidence_mat_tensor = batch\n",
    "    output = conv_net_features(image_tensor)\n",
    "    count+=1\n",
    "    print(\"-----------------\\n-------------------\\n\")\n",
    "    if count==1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 147, 410])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 323])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_mat_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence_mat_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace=True)\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace=True)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
