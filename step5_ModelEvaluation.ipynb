{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"CUDatamap_line2.pkl\",\"rb\") as f:\n",
    "    data_line_map = pickle.load(f)\n",
    "with open(\"CUDatamap_stline2.pkl\",\"rb\") as f:\n",
    "    data_stline_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = np.linspace(400,580,19)\n",
    "\n",
    "class LaneDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_line_map,data_stline_map,scale=None,reg_scale=10):\n",
    "\n",
    "        self.new_data_keys = list(data_line_map.keys())\n",
    "        self.data_line_map = data_line_map\n",
    "        self.data_stline_map = data_stline_map\n",
    "        self.scale = scale\n",
    "        image_loc = self.new_data_keys[0]\n",
    "        image = cv2.imread(image_loc)\n",
    "        height,width,channel = image.shape\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.reg_scale = reg_scale\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.new_data_keys)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_lane_angle(self,myline):\n",
    "        global y_col #its actually row\n",
    "        row = y_col[-1]\n",
    "        col = myline[-1]\n",
    "\n",
    "        new_col = col - self.width//2\n",
    "        new_row = row + self.height\n",
    "\n",
    "        angle = -np.arctan2(new_col,new_row)\n",
    "        # if angle<0:\n",
    "        #     angle = 2*np.pi+angle\n",
    "\n",
    "        return angle\n",
    "\n",
    "    def get_lane_bins(self,stline):\n",
    "        angle = self.get_lane_angle(stline)\n",
    "        angle = np.clip(angle,-1,1)\n",
    "        mybin = int(round(angle/0.125))+8 #17 bins -1 to 1 as 0 to 17\n",
    "\n",
    "        return mybin,angle\n",
    "\n",
    "\n",
    "    def get_regression_confidence_mat(self,stlines,lines):\n",
    "        regression_mat = np.zeros((19,17),dtype=np.float32) #19 rows and 17 for 17 bins\n",
    "        confidence_mat = np.zeros((17,),dtype=np.float32)\n",
    "        for stline,line in zip(stlines,lines):\n",
    "            mybin,angle = self.get_lane_bins(stline=stline)\n",
    "            regression_mat[:,mybin] = line\n",
    "            confidence_mat[mybin] = 1\n",
    "        return regression_mat.flatten(),confidence_mat\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        image_loc = self.new_data_keys[idx]\n",
    "        lines = self.data_line_map[image_loc] #Since\n",
    "        image = cv2.imread(image_loc)\n",
    "        image_org = image.copy()\n",
    "\n",
    "        if self.scale is not None:\n",
    "            image = cv2.resize(image,self.scale)\n",
    "\n",
    "        stlines = self.data_stline_map[image_loc]\n",
    "\n",
    "\n",
    "        #visualize_bundle = (image,lines,stlines,idx) #use only for visualization\n",
    "\n",
    "\n",
    "        image_tensor = torch.tensor(image,dtype=torch.float32)/255 #convert to tensor and normalise\n",
    "        image_tensor2 = image_tensor.permute(2,0,1) #bring the channel to front\n",
    "\n",
    "        regression_mat,confidence_mat  = self.get_regression_confidence_mat(stlines,lines)\n",
    "        regression_mat_tensor = torch.tensor(regression_mat,dtype=torch.float32)\n",
    "        regression_mat_tensor = torch.clip(regression_mat_tensor,-1500,2500)/self.reg_scale #Clip and also reduce the scale\n",
    "        confidence_mat_tensor = torch.tensor(confidence_mat,dtype=torch.float32)\n",
    "\n",
    "        #print(image_loc,sum(regression_mat))\n",
    "\n",
    "\n",
    "        return image_tensor2,regression_mat_tensor,confidence_mat_tensor,image_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lane_dataset = LaneDataset(data_line_map=data_line_map,data_stline_map=data_stline_map,scale=(224,224))\n",
    "dataloader = DataLoader(lane_dataset, batch_size=1,shuffle=False) #One for one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshayd/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/akshayd/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn import functional as F\n",
    "class VGGBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGBlock, self).__init__()\n",
    "        self.block1 = nn.Sequential(*list(vgg16.features.children())[:5])\n",
    "        self.block1t4 = nn.Sequential(*list(vgg16.features.children())[5:16])  # Block 4 of VGG16\n",
    "\n",
    "    def forward(self, x):\n",
    "      x1_b = self.block1(x)  # Output will be of shape (b, 64, 112, 112)\n",
    "      x2 = self.block1t4(x1_b)  # Output will be of shape (b, 256, 56, 56)\n",
    "      return x1_b,x2\n",
    "\n",
    "\n",
    "class UpSampleNeck(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UpSampleNeck, self).__init__()\n",
    "        # Trainable 3x3 convolution to reduce features from 64 to 32\n",
    "        self.conv3x3_1 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # BatchNorm after first conv\n",
    "\n",
    "        # Reduce Block 4 features from 256 to 128\n",
    "        self.reduce_conv_block4 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)  # BatchNorm after Block 4 reduction\n",
    "\n",
    "        # Upsample\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)  # BatchNorm after upconv\n",
    "\n",
    "        # Convolution layers after concatenation\n",
    "        self.conv3x3_2 = nn.Conv2d(in_channels=96, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)  # BatchNorm after concat\n",
    "        self.avg_pool_2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "    def forward(self,x1_b,x2):\n",
    "        # Pass through the 3x3 convolution layer, batch norm, and ReLU\n",
    "        x1 = self.conv3x3_1(x1_b)  # Output will be of shape (b, 32, 112, 112)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "\n",
    "        # Block 4 forward pass (without max-pooling)\n",
    "        x2 = self.reduce_conv_block4(x2)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = self.upconv(x2)  # Output will be of shape (b, 64, 112, 112)\n",
    "        x2 = self.bn3(x2)\n",
    "        x2 = F.relu(x2)\n",
    "\n",
    "        # Concatenate x1 and x2 along the channel dimension\n",
    "        x_concat = torch.cat((x1, x2), dim=1)  # Output will be of shape (b, 96, 112, 112)\n",
    "\n",
    "        # Convolution layer after concatenation\n",
    "        x3 = self.conv3x3_2(x_concat)\n",
    "        x3 = self.bn4(x3)\n",
    "        x3 = self.avg_pool_2(x3)  # Output will be of shape (b, 128, 56, 56)\n",
    "\n",
    "        x5 = F.relu(x3) #Will be also used for confidence calculation\n",
    "\n",
    "        return x5,x_concat\n",
    "\n",
    "\n",
    "class LaneConfidenceHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LaneConfidenceHead, self).__init__()\n",
    "\n",
    "        self.conv3x3_9 = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,stride=1)\n",
    "        self.bn11 = nn.BatchNorm2d(256)\n",
    "        self.maxpool_5 = nn.MaxPool2d(kernel_size=3,stride=3)\n",
    "\n",
    "        self.conv3x3_10 = nn.Conv2d(in_channels=256,out_channels=64,kernel_size=1,stride=1)\n",
    "        self.bn12 = nn.BatchNorm2d(64)\n",
    "        #self.maxpool_6 = nn.MaxPool2d(kernel_size=3,stride=3)\n",
    "\n",
    "        self.conv3x3_11 = nn.Conv2d(in_channels=64,out_channels=8,kernel_size=1,stride=1)\n",
    "        self.bn13 = nn.BatchNorm2d(8)\n",
    "        #self.maxpool_7 = nn.MaxPool2d(kernel_size=3,stride=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.Linear_c1 = nn.Linear(in_features=2592,out_features=256)\n",
    "        self.Linear_c2 = nn.Linear(in_features=256,out_features=17)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "\n",
    "    def forward(self,x5):\n",
    "\n",
    "\n",
    "      x5 = self.conv3x3_9(x5)\n",
    "      x5 = self.bn11(x5)\n",
    "      x5 = self.maxpool_5(x5)\n",
    "      x5 = F.relu(x5)\n",
    "\n",
    "\n",
    "\n",
    "      x5 = self.conv3x3_10(x5)\n",
    "      x5 = self.bn12(x5)\n",
    "      #x5 = self.maxpool_6(x5)\n",
    "      x5 = F.relu(x5)\n",
    "\n",
    "\n",
    "\n",
    "      x5 = self.conv3x3_11(x5)\n",
    "      x5 = self.bn13(x5)\n",
    "\n",
    "      #x5 = self.maxpool_7(x5)\n",
    "      x5 = F.relu(x5)\n",
    "\n",
    "      x5 = self.flatten(x5)\n",
    "\n",
    "\n",
    "\n",
    "      x5 = self.Linear_c1(x5)\n",
    "      x5 = F.sigmoid(x5)\n",
    "      x5 = self.Linear_c2(x5)\n",
    "\n",
    "\n",
    "      return x5\n",
    "\n",
    "# Add positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,device, d_model, max_len=256):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)  # Add batch dimension\n",
    "        self.encoding = self.encoding.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return x + self.encoding[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class Regression_Head(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(Regression_Head, self).__init__()\n",
    "        self.conv3x3_12 = nn.Conv2d(in_channels=96,out_channels=12,kernel_size=1,stride=1,padding=1)\n",
    "        # Patch extraction using unfold\n",
    "        self.patch_size = 7\n",
    "        self.unfold = nn.Unfold(kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        self.fold = nn.Fold(output_size=(112, 112), kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        num_patches = 256  # Number of patches\n",
    "        embedding_dim = 588  # Each patch dimension\n",
    "        channels = 12\n",
    "        self.patch_embedding_layer = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(device,embedding_dim, max_len=num_patches)\n",
    "\n",
    "        # Define multi-head attention layer\n",
    "        num_heads = 2\n",
    "        self.attention_layer = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads, dropout=0.1)\n",
    "\n",
    "\n",
    "        self.Conv2d_13 = nn.Conv2d(in_channels=12,out_channels=6,kernel_size=2,stride=2)\n",
    "        self.bn_14 = nn.BatchNorm2d(6)\n",
    "        self.Conv2d_15 = nn.Conv2d(in_channels=6,out_channels=2,kernel_size=2,stride=2)\n",
    "        self.bn_16 = nn.BatchNorm2d(2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_final = nn.Linear(in_features=1568,out_features=512)\n",
    "        self.linear_final2 = nn.Linear(in_features=512,out_features=19*17)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x_concat):\n",
    "        x6 = self.conv3x3_12(x_concat)\n",
    "        x6 = F.relu(x6)\n",
    "\n",
    "        patches = self.unfold(x6) # Shape: [batch_size,channels * patch_size * patch_size, num_patches]\n",
    "        #batch_size = patches.size(0)\n",
    "        #channels = patches.size(1)\n",
    "        #print(patches.shape)\n",
    "        patches = patches.permute(0, 2,1)\n",
    "\n",
    "        patches = self.patch_embedding_layer(patches)\n",
    "        #print(\"After Patch Embedding\",patches.shape)\n",
    "        patches = self.positional_encoding(patches)\n",
    "        #print(\"After Patch Embedding\",patches.shape)\n",
    "\n",
    "        # Apply multi-head attention\n",
    "        attention_output, _ = self.attention_layer(patches, patches, patches)\n",
    "        #print(\"After Attention\",attention_output.shape)\n",
    "\n",
    "        # Fold the attention output back to the original image size\n",
    "        attention_output = attention_output.permute(0, 2, 1)\n",
    "        transformed_feature_map = self.fold(attention_output)\n",
    "        #print(\"After Fold\",transformed_feature_map.shape)\n",
    "\n",
    "        conv_out = self.Conv2d_13(transformed_feature_map)\n",
    "        conv_out = self.bn_14(conv_out)\n",
    "        cov_out = F.relu(conv_out)\n",
    "\n",
    "        conv_out = self.Conv2d_15(cov_out)\n",
    "        conv_out = self.bn_16(conv_out)\n",
    "        conv_out = F.relu(conv_out)\n",
    "\n",
    "        conv_out = self.flatten(conv_out)\n",
    "        conv_out = self.linear_final(conv_out)\n",
    "        conv_out = F.relu(conv_out)\n",
    "        conv_out = self.linear_final2(conv_out)\n",
    "\n",
    "        return conv_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LaneDetection_2(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(LaneDetection_2, self).__init__()\n",
    "\n",
    "        self.vggblock = VGGBlock()\n",
    "        self.upsample_neck = UpSampleNeck()\n",
    "        self.laneconfidence_head = LaneConfidenceHead()\n",
    "\n",
    "        self.regression_head = Regression_Head(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Block 1 forward pass\n",
    "        x1_b,x2 = self.vggblock(x)\n",
    "\n",
    "        x5,x_concat = self.upsample_neck(x1_b,x2)\n",
    "\n",
    "        #print(x5.shape,x_concat.shape)\n",
    "\n",
    "        x6 = self.laneconfidence_head(x5)\n",
    "\n",
    "        x7 = self.regression_head(x_concat)\n",
    "\n",
    "        return x6,x7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('LaneViT_regression.pt', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Stop here",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStop here\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Stop here"
     ]
    }
   ],
   "source": [
    "assert False,\"Stop here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m,_,_,image \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot one more\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     confidence_pred,regression_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     confidence_pred \u001b[38;5;241m=\u001b[39m sig(confidence_pred) \u001b[38;5;66;03m#Get the confidence as its a logit \u001b[39;00m\n\u001b[1;32m      8\u001b[0m     colors \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m:(\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m),\u001b[38;5;241m1\u001b[39m:(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m0\u001b[39m),\u001b[38;5;241m2\u001b[39m:(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m255\u001b[39m),\u001b[38;5;241m3\u001b[39m:(\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m255\u001b[39m,\u001b[38;5;241m255\u001b[39m)}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 232\u001b[0m, in \u001b[0;36mLaneDetection_2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m# Block 1 forward pass\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m     x1_b,x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvggblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     x5,x_concat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample_neck(x1_b,x2)\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m#print(x5.shape,x_concat.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mVGGBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m   x1_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Output will be of shape (b, 64, 112, 112)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m   x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock1t4(x1_b)  \u001b[38;5;66;03m# Output will be of shape (b, 256, 56, 56)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x1_b,x2\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "count = 10000\n",
    "sig = nn.Sigmoid()\n",
    "for input,_,_,image in dataloader:\n",
    "    print(\"Got one more\")\n",
    "    confidence_pred,regression_pred = model(input)\n",
    "    confidence_pred = sig(confidence_pred) #Get the confidence as its a logit \n",
    "    colors = {0:(255,0,0),1:(0,255,0),2:(0,0,255),3:(255,255,255)}\n",
    "    lines = []\n",
    "    mybins = []\n",
    "    confidence = list(confidence_pred.detach().numpy()[0,:].round())\n",
    "    regression = regression_pred.detach().numpy()[0,:].reshape(19,17)*lane_dataset.reg_scale\n",
    "    regco = []\n",
    "    for i in range(len(confidence)):\n",
    "        regco.append([confidence[i],regression[:,i]])\n",
    "    \n",
    "    regco.sort(key=itemgetter(0))\n",
    "\n",
    "    for i in range(len(confidence)):\n",
    "        if round(regco[i][0])==1:\n",
    "            lines.append(regco[i][1])\n",
    "            mybins.append(i)\n",
    "    \n",
    "    image = image.detach().numpy()[0].astype(np.uint8)\n",
    "\n",
    "    print(\"Model predicted, procedding to draw\",mybins)\n",
    "\n",
    "    for j,l in enumerate(lines[0:4]):\n",
    "        my_line = l.astype(int)\n",
    "        bin_no = mybins[j]\n",
    "        line_length = len(l)\n",
    "        for i,p in enumerate(my_line):\n",
    "            cv2.circle(image, (p, int(y_col[i])), i+1, colors[j], -1)\n",
    "            if(i==(line_length-10)):\n",
    "                cv2.putText(image,str(bin_no),org=(p,int(y_col[i])),fontFace=cv2.FONT_HERSHEY_COMPLEX,fontScale=3,color=(255,255,0),thickness=3)\n",
    "\n",
    "\n",
    "    print(\"Drew\",input.shape)\n",
    "  \n",
    "    count+=1\n",
    "    cv2.imwrite(\"results/\"+str(count)+\".jpg\",image)\n",
    "#The thing finally works, I am so happy :)))))))))))))))))\n",
    "#https://huggingface.co/docs/transformers/model_doc/detr we will try to use this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading custom video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LaneDataset2(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, video_file:str):\n",
    "                # Load the video\n",
    "    \n",
    "        self.cap = cv2.VideoCapture(video_file)\n",
    "        if not self.cap.isOpened():\n",
    "            print(\"Error: Could not open video.\")\n",
    "            exit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        self.cap.release()\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            raise FileNotFoundError\n",
    "        resized_224x224 = cv2.resize(frame, (224, 224))\n",
    "        resized_big = cv2.resize(frame, (1640,590))\n",
    "\n",
    "        image_tensor = torch.tensor(resized_224x224,dtype=torch.float32)/255 #convert to tensor and normalise\n",
    "        image_tensor2 = image_tensor.permute(2,0,1) #bring the channel to front\n",
    "\n",
    "\n",
    "        return image_tensor2,resized_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lane_dataset = LaneDataset2(\"lanehighway.mp4\")\n",
    "dataloader = DataLoader(lane_dataset, batch_size=1,shuffle=False) #One for one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n",
      "Model predicted, procedding to draw [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Drew torch.Size([1, 3, 224, 224])\n",
      "Got one more\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "count = 10000\n",
    "sig = nn.Sigmoid()\n",
    "for input,image in dataloader:\n",
    "    print(\"Got one more\")\n",
    "    confidence_pred,regression_pred = model(input)\n",
    "    confidence_pred = sig(confidence_pred) #Get the confidence as its a logit \n",
    "    colors = {0:(255,0,0),1:(0,255,0),2:(0,0,255),3:(255,255,255)}\n",
    "    lines = []\n",
    "    mybins = []\n",
    "    confidence = list(confidence_pred.detach().numpy()[0,:].round())\n",
    "    regression = regression_pred.detach().numpy()[0,:].reshape(19,17)*10\n",
    "    regco = []\n",
    "    for i in range(len(confidence)):\n",
    "        regco.append([confidence[i],regression[:,i]])\n",
    "    \n",
    "    regco.sort(key=itemgetter(0),reverse=True)\n",
    "\n",
    "    for i in range(len(confidence)):\n",
    "        #if round(regco[i][0])==1:\n",
    "        lines.append(regco[i][1])\n",
    "        mybins.append(i)\n",
    "    \n",
    "    image = image.detach().numpy()[0].astype(np.uint8)\n",
    "\n",
    "    print(\"Model predicted, procedding to draw\",mybins)\n",
    "\n",
    "    for j,l in enumerate(lines[0:4]):\n",
    "        my_line = l.astype(int)\n",
    "        bin_no = mybins[j]\n",
    "        line_length = len(l)\n",
    "        for i,p in enumerate(my_line):\n",
    "            cv2.circle(image, (p, int(y_col[i])), i+1, colors[j], -1)\n",
    "            if(i==(line_length-10)):\n",
    "                cv2.putText(image,str(bin_no),org=(p,int(y_col[i])),fontFace=cv2.FONT_HERSHEY_COMPLEX,fontScale=3,color=(255,255,0),thickness=3)\n",
    "\n",
    "\n",
    "    print(\"Drew\",input.shape)\n",
    "  \n",
    "    count+=1\n",
    "    cv2.imwrite(\"results/\"+str(count)+\".jpg\",image)\n",
    "#The thing finally works, I am so happy :)))))))))))))))))\n",
    "#https://huggingface.co/docs/transformers/model_doc/detr we will try to use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "regco.sort(key=itemgetter(0),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0,\n",
       "  array([ 830.0215 ,  860.8435 ,  893.28094,  925.56323,  957.8702 ,\n",
       "          990.82336, 1023.1421 , 1054.845  , 1086.2734 , 1118.5061 ,\n",
       "         1150.8606 , 1183.5614 , 1217.0872 , 1249.9808 , 1284.5808 ,\n",
       "         1319.8224 , 1354.7776 , 1392.0265 , 1426.2522 ], dtype=float32)],\n",
       " [1.0,\n",
       "  array([696.24133, 686.55975, 678.3297 , 668.6559 , 659.2444 , 647.9546 ,\n",
       "         637.929  , 626.19617, 614.451  , 601.91644, 591.09973, 579.4304 ,\n",
       "         566.83856, 555.2695 , 542.89215, 530.8552 , 518.73376, 508.47995,\n",
       "         496.97742], dtype=float32)],\n",
       " [0.0,\n",
       "  array([1010.458 , 1090.1995, 1076.5541, 1095.7008, 1176.4377, 1334.9443,\n",
       "         1509.8718, 1592.3712, 1675.6603, 1757.9766, 1840.3389, 1921.0837,\n",
       "         2001.5713, 2080.555 , 2159.3896, 2239.1887, 2302.2327, 2355.9617,\n",
       "         2403.0679], dtype=float32)],\n",
       " [0.0,\n",
       "  array([ 814.176 ,  887.2794,  955.8307, 1021.2803, 1089.9142, 1157.8887,\n",
       "         1227.6104, 1297.8502, 1368.7085, 1442.8003, 1521.5297, 1594.6251,\n",
       "         1672.1521, 1760.2944, 1851.4182, 1933.1104, 1994.3916, 2035.4524,\n",
       "         2071.0002], dtype=float32)],\n",
       " [0.0,\n",
       "  array([ 846.5901 ,  901.5137 ,  955.81726, 1010.4635 , 1064.406  ,\n",
       "         1119.373  , 1172.6818 , 1226.5037 , 1280.7404 , 1334.7932 ,\n",
       "         1389.3654 , 1442.4844 , 1495.3425 , 1546.6633 , 1597.8752 ,\n",
       "         1650.6736 , 1705.8898 , 1761.2152 , 1817.4114 ], dtype=float32)],\n",
       " [0.0,\n",
       "  array([ 805.34766,  849.42017,  894.4525 ,  939.8664 ,  985.74133,\n",
       "         1030.8223 , 1076.5815 , 1122.2927 , 1166.7998 , 1211.8488 ,\n",
       "         1255.6953 , 1299.4023 , 1343.1418 , 1386.1543 , 1428.8284 ,\n",
       "         1472.2888 , 1514.6984 , 1557.3501 , 1599.8632 ], dtype=float32)],\n",
       " [0.0,\n",
       "  array([ 782.51965,  811.38666,  840.12335,  867.2071 ,  893.6505 ,\n",
       "          919.9105 ,  946.3439 ,  971.72064,  996.73456, 1021.409  ,\n",
       "         1047.4572 , 1073.065  , 1097.683  , 1122.9648 , 1146.8322 ,\n",
       "         1172.0342 , 1197.5935 , 1223.452  , 1249.8398 ], dtype=float32)],\n",
       " [0.0,\n",
       "  array([ 755.2479 ,  773.088  ,  792.64496,  809.39514,  827.5271 ,\n",
       "          844.7922 ,  861.67633,  880.0725 ,  896.87195,  915.0971 ,\n",
       "          932.39905,  950.42175,  968.41785,  985.6945 , 1003.48114,\n",
       "         1021.47815, 1038.4548 , 1056.3455 , 1073.8518 ], dtype=float32)],\n",
       " [0.0,\n",
       "  array([746.6941 , 758.0817 , 769.42883, 780.2776 , 792.2974 , 804.78424,\n",
       "         816.7385 , 828.93   , 841.91956, 853.6583 , 865.14905, 878.2248 ,\n",
       "         889.7361 , 902.52814, 914.5911 , 926.4642 , 939.50146, 951.6727 ,\n",
       "         964.04877], dtype=float32)],\n",
       " [0.0,\n",
       "  array([671.5268 , 677.2101 , 681.54004, 687.5231 , 693.2938 , 700.3463 ,\n",
       "         706.65173, 712.7958 , 720.444  , 726.65027, 733.90735, 741.10803,\n",
       "         748.1261 , 754.2168 , 760.6396 , 767.05383, 771.457  , 778.1139 ,\n",
       "         782.73145], dtype=float32)],\n",
       " [0.0,\n",
       "  array([716.5292 , 712.7197 , 707.99554, 702.7648 , 698.28503, 694.81995,\n",
       "         690.55   , 685.76733, 682.3678 , 679.5902 , 675.08905, 671.76   ,\n",
       "         668.32117, 664.5088 , 660.25824, 656.1288 , 651.4969 , 647.8019 ,\n",
       "         643.89954], dtype=float32)],\n",
       " [0.0,\n",
       "  array([719.4489 , 701.1749 , 682.459  , 664.1452 , 646.1953 , 628.11914,\n",
       "         610.1096 , 591.761  , 573.3027 , 554.34625, 534.9802 , 516.2939 ,\n",
       "         497.48145, 478.1272 , 459.33197, 439.54706, 418.65384, 398.52258,\n",
       "         378.31616], dtype=float32)],\n",
       " [0.0,\n",
       "  array([667.2708 , 635.5027 , 604.03253, 575.147  , 549.0408 , 521.7708 ,\n",
       "         495.48834, 469.61118, 441.03308, 413.6192 , 385.12946, 356.7312 ,\n",
       "         327.7982 , 299.95874, 272.0161 , 244.2234 , 216.85938, 188.86162,\n",
       "         162.33664], dtype=float32)],\n",
       " [0.0,\n",
       "  array([661.71686  , 623.40826  , 585.1822   , 547.1616   , 507.78122  ,\n",
       "         469.65845  , 430.7474   , 392.21716  , 352.925    , 312.5971   ,\n",
       "         272.52863  , 232.45401  , 192.59538  , 151.82466  , 114.33455  ,\n",
       "          77.08302  ,  39.332844 ,   2.1094294, -37.11512  ], dtype=float32)],\n",
       " [0.0,\n",
       "  array([ 637.216   ,  592.99615 ,  546.9398  ,  500.51788 ,  452.28204 ,\n",
       "          404.88235 ,  356.4541  ,  309.0022  ,  260.37817 ,  211.44258 ,\n",
       "          164.32936 ,  116.13723 ,   68.33043 ,   21.319069,  -26.017473,\n",
       "          -73.514786, -117.49513 , -163.22816 , -206.39423 ], dtype=float32)],\n",
       " [0.0,\n",
       "  array([ 548.1998   ,  484.59552  ,  420.11227  ,  357.02643  ,\n",
       "          294.46118  ,  232.41348  ,  171.44547  ,  111.75778  ,\n",
       "           53.556576 ,   -2.4525952,  -58.10896  , -111.07024  ,\n",
       "         -164.94565  , -215.24542  , -269.10403  , -322.4318   ,\n",
       "         -377.29425  , -431.97647  , -488.70602  ], dtype=float32)],\n",
       " [0.0,\n",
       "  array([  519.4789   ,   447.74765  ,   374.86478  ,   294.13922  ,\n",
       "           206.31476  ,   110.257385 ,     1.7682445,  -115.32988  ,\n",
       "          -245.42252  ,  -391.84283  ,  -561.06104  ,  -755.7263   ,\n",
       "          -976.16766  , -1039.4304   , -1093.7185   , -1130.9811   ,\n",
       "         -1165.1571   , -1194.7145   , -1208.4567   ], dtype=float32)]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
